{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, h5py, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.signal import butter, lfilter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting constants and directories ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = \"\" # Directory of this script\n",
    "output_dir = os.path.join(script_dir, \"output\") # Directory where the output will be saved\n",
    "output_filepath = os.path.join(output_dir, 'Action_train.pkl')\n",
    "\n",
    "data_input_dir = \"\" # Directory of the input files\n",
    "cap = cv2.VideoCapture(os.path.join(data_input_dir, \"actionnet_S04.mp4\"))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cap.release()\n",
    "\n",
    "baseline_label = \"None\"\n",
    "activities_to_classify = [\n",
    "  baseline_label,\n",
    "  'Get/replace items from refrigerator/cabinets/drawers',\n",
    "  'Peel a cucumber',\n",
    "  'Clear cutting board',\n",
    "  'Slice a cucumber',\n",
    "  'Peel a potato',\n",
    "  'Slice a potato',\n",
    "  'Slice bread',\n",
    "  'Spread almond butter on a bread slice',\n",
    "  'Spread jelly on a bread slice',\n",
    "  'Open/close a jar of almond butter',\n",
    "  'Pour water from a pitcher into a glass',\n",
    "  'Clean a plate with a sponge',\n",
    "  'Clean a plate with a towel',\n",
    "  'Clean a pan with a sponge',\n",
    "  'Clean a pan with a towel',\n",
    "  'Get items from cabinets: 3 each large/small plates, bowls, mugs, glasses, sets of utensils',\n",
    "  'Set table: 3 each large/small plates, bowls, mugs, glasses, sets of utensils',\n",
    "  'Stack on table: 3 each large/small plates, bowls',\n",
    "  'Load dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils',\n",
    "  'Unload dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils',\n",
    " ]\n",
    "baseline_index = activities_to_classify.index(baseline_label)\n",
    "\n",
    "resampled_Fs = fps * 5\n",
    "num_segments_per_subject = 20\n",
    "segment_duration_s = 2.5\n",
    "segment_length = int(round(resampled_Fs * segment_duration_s))\n",
    "buffer_startActivity_s = 2\n",
    "buffer_endActivity_s = 2\n",
    "\n",
    "filter_cutoff_emg_Hz = 5\n",
    "\n",
    "devices = ['myo-left', 'myo-right']\n",
    "stream_name = 'emg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of utility functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(data, cutoff, Fs, order=5):\n",
    "    nyq = 0.5 * Fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog = False)\n",
    "    y = lfilter(b, a, data.T).T\n",
    "    return y\n",
    "\n",
    "def get_feature_matrix(experiment_data, experiment_times, label_start_time_s, label_end_time_s, count=num_segments_per_subject):\n",
    "    start_time_s = label_start_time_s + buffer_startActivity_s\n",
    "    end_time_s = label_end_time_s - buffer_endActivity_s\n",
    "    if end_time_s - start_time_s > buffer_startActivity_s + buffer_endActivity_s + segment_duration_s:\n",
    "        segment_start_times_s = np.linspace(start_time_s, end_time_s - segment_duration_s, num=count, endpoint=True)\n",
    "    else:\n",
    "        segment_start_times_s = np.linspace(label_start_time_s, label_end_time_s, num=count, endpoint=True)\n",
    "    segment_indexes = np.int32(np.round((segment_start_times_s - label_start_time_s)*fps))\n",
    "    feature_matrices = []\n",
    "    for segment_start_time_s in segment_start_times_s:\n",
    "        segment_end_time_s = segment_start_time_s + segment_duration_s\n",
    "        feature_matrix = np.empty(shape=(segment_length, 0))\n",
    "        for myo_key in devices:\n",
    "            data = np.squeeze(np.array(experiment_data[myo_key]))\n",
    "            time_s = np.squeeze(np.array(experiment_times[myo_key]))\n",
    "            time_indexes = np.where((time_s >= segment_start_time_s) & (time_s <= segment_end_time_s))[0]\n",
    "            time_indexes = list(time_indexes)\n",
    "            \n",
    "            while len(time_indexes) < segment_length:\n",
    "                #print(\"Increasing segment length\")\n",
    "                if time_indexes[0] > 0:\n",
    "                    time_indexes = [time_indexes[0] - 1] + time_indexes\n",
    "                elif time_indexes[-1] < len(time_s) - 1:\n",
    "                    time_indexes.append(time_indexes[-1] + 1)\n",
    "                else:\n",
    "                    raise AssertionError\n",
    "            while len(time_indexes) > segment_length:\n",
    "                #print(\"Decreasing segment length\")\n",
    "                time_indexes.pop()\n",
    "            time_indexes = np.array(time_indexes)\n",
    "            \n",
    "            extraction = lambda data : data\n",
    "            time_s = time_s[time_indexes]\n",
    "            data = data[time_indexes, :]\n",
    "            data = extraction(data)\n",
    "            data = np.reshape(data, (segment_length, -1))\n",
    "            \n",
    "            feature_matrix = np.concatenate((feature_matrix, data), axis=1)\n",
    "        feature_matrices.append(feature_matrix)\n",
    "    return feature_matrix, segment_indexes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EMG Data + Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "time = {}\n",
    "\n",
    "hdf_filepath = os.path.join(data_input_dir, 'actionnet_wearables_S04.hdf5')\n",
    "hdf_file = h5py.File(hdf_filepath, 'r')\n",
    "\n",
    "for device_name in devices:\n",
    "    data[device_name] = hdf_file[device_name][stream_name]['data'][:]\n",
    "    time[device_name] = hdf_file[device_name][stream_name]['time_s'][:]\n",
    "\n",
    "\n",
    "for myo_key in devices:\n",
    "    #Filter\n",
    "    t = time[myo_key]\n",
    "    Fs = (t.size - 1) / (t[-1] - t[0])\n",
    "    data_stream = data[myo_key][:, :]\n",
    "    y = np.abs(data_stream)\n",
    "    y = lowpass_filter(y, filter_cutoff_emg_Hz, Fs)\n",
    "    #Normalize\n",
    "    y = y / ((np.amax(y) - np.amin(y)) / 2)\n",
    "    y = y - np.amin(y) - 1\n",
    "    #Resample\n",
    "    squeezed_data = np.squeeze(np.array(y))\n",
    "    time_s = np.squeeze(np.array(time[myo_key]))\n",
    "    target_time_s = np.linspace(time_s[0], time_s[-1], num=int(round(1+resampled_Fs*(time_s[-1] - time_s[0]))), endpoint=True)\n",
    "    fn_interpolate = interpolate.interp1d(time_s,squeezed_data,axis=0,kind='linear',fill_value='extrapolate')\n",
    "    data_resampled = fn_interpolate(target_time_s)\n",
    "    \n",
    "    data[myo_key] = data_resampled\n",
    "    time[myo_key] = target_time_s\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating examples for EMG data and corresponding indices for the RGB clips ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_matrices_byLabel = {}\n",
    "example_frames_byLabel = {}\n",
    "\n",
    "example_matrices = []\n",
    "example_frames = []\n",
    "example_labels = []\n",
    "example_label_indexes = []\n",
    "\n",
    "noActivity = []\n",
    "noActivity_frames = []\n",
    "\n",
    "video_ids = []\n",
    "\n",
    "activities = hdf_file['experiment-activities']['activities']['data']\n",
    "activities = [[x.decode('utf-8') for x in datas] for datas in activities]\n",
    "\n",
    "activities_times = hdf_file['experiment-activities']['activities']['time_s']\n",
    "activities_times = np.squeeze(np.array(activities_times))\n",
    "\n",
    "activities_labels = []\n",
    "activities_start_times_s = []\n",
    "activities_end_times_s = []\n",
    "\n",
    "\n",
    "for (row_index, time_s) in enumerate(activities_times):\n",
    "    label = activities[row_index][0]\n",
    "    is_start = activities[row_index][1] == 'Start'\n",
    "    is_stop = activities[row_index][1] == 'Stop'\n",
    "    rating = activities[row_index][2]\n",
    "    \n",
    "    if rating in ['Bad', 'Maybe']:\n",
    "        continue\n",
    "    if is_start:\n",
    "        activities_labels.append(label)\n",
    "        activities_start_times_s.append(time_s)\n",
    "    if is_stop:\n",
    "        activities_end_times_s.append(time_s)\n",
    "for (label_index, activity_label) in enumerate(activities_to_classify):\n",
    "    if label_index == baseline_index:\n",
    "        continue\n",
    "    file_label_indexes = [i for (i,label) in enumerate(activities_labels) if label==activity_label]\n",
    "    for file_label_index in file_label_indexes:\n",
    "        start_time_s = activities_start_times_s[file_label_index]\n",
    "        end_time_s = activities_end_times_s[file_label_index]\n",
    "        duration_s = end_time_s - start_time_s\n",
    "        \n",
    "        video_ids.append(f\"{file_label_index}/{file_label_index}\")\n",
    "        feature_matrices, segment_indexes = get_feature_matrix(data, time, start_time_s, end_time_s, count=num_segments_per_subject)\n",
    "        example_frames.append(segment_indexes)\n",
    "        example_matrices_byLabel.setdefault(activity_label, [])\n",
    "        example_frames_byLabel.setdefault(activity_label, [])\n",
    "        example_matrices_byLabel[activity_label].append(feature_matrices)\n",
    "        example_frames_byLabel[activity_label].append(segment_indexes)\n",
    "\n",
    "for (label_index, activity_label) in enumerate(activities_labels):\n",
    "    if label_index == len(activities_labels) - 1:\n",
    "        continue\n",
    "    noActivity_start_time_s = activities_end_times_s[label_index]\n",
    "    noActivity_end_time_s = activities_start_times_s[label_index + 1]\n",
    "    duration_s = noActivity_end_time_s - noActivity_start_time_s\n",
    "    if duration_s < 10:\n",
    "        continue\n",
    "    video_ids.append(f\"None_{label_index}/None_{label_index}\")\n",
    "    feature_matrices, segment_indexes = get_feature_matrix(data, time, noActivity_start_time_s, noActivity_end_time_s, count=10)\n",
    "    noActivity.append(feature_matrices)\n",
    "    noActivity_frames.append(segment_indexes)\n",
    "    \n",
    "for (activity_label_index, activity_label) in enumerate(activities_to_classify):\n",
    "    if activity_label_index == baseline_index:\n",
    "        continue\n",
    "    feature_matrices = example_matrices_byLabel[activity_label]\n",
    "    example_indexes = np.round(np.linspace(0, len(feature_matrices)-1, endpoint=True, num=num_segments_per_subject, dtype=int))\n",
    "    for example_index in example_indexes:\n",
    "        example_labels.append(activity_label)\n",
    "        example_label_indexes.append(activity_label_index)\n",
    "        example_matrices.append(feature_matrices[example_index])\n",
    "\n",
    "noActivity_indexes = np.round(np.linspace(0, len(noActivity)-1, endpoint=True, num=num_segments_per_subject, dtype=int))\n",
    "for noActivity_index in noActivity_indexes:\n",
    "    example_labels.append(baseline_label)\n",
    "    example_label_indexes.append(baseline_index)\n",
    "    example_matrices.append(noActivity[noActivity_index])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of starting frames indices for the RGB clips ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "\n",
    "\n",
    "for label in activities_to_classify:\n",
    "    if label == baseline_label:\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        segments = example_frames_byLabel[label]\n",
    "        segments_flat = np.array(segments).flatten()\n",
    "        indexes = np.round(np.linspace(0, len(segments_flat)-1, num=num_segments_per_subject, endpoint=True, dtype=int))\n",
    "        segments_perLabel = []\n",
    "        for i in range(len(segments)):\n",
    "            aux = []\n",
    "            for index in indexes:\n",
    "                index = index - i*num_segments_per_subject\n",
    "                if index < 0 or index >= num_segments_per_subject:\n",
    "                    continue\n",
    "                else:\n",
    "                    aux.append(np.array(segments)[i][index] + 1)\n",
    "            segments_perLabel.append(aux)\n",
    "            \n",
    "        all_segments.append(segments_perLabel)\n",
    "\n",
    "label = baseline_label\n",
    "segments = np.array(noActivity_frames)\n",
    "segments_flat = segments.flatten()\n",
    "indexes = np.round(np.linspace(0, len(segments_flat)-1, num=num_segments_per_subject, endpoint=True, dtype=int))\n",
    "segments_perLabel = []\n",
    "for i in range(len(segments)):\n",
    "    aux = []\n",
    "    for index in indexes:\n",
    "        index = index - i * 10\n",
    "        if index < 0 or index >= 10:\n",
    "            continue\n",
    "        else:\n",
    "            aux.append(np.array(segments)[i][index] + 1)    \n",
    "    segments_perLabel.append(aux)\n",
    "all_segments.append(segments_perLabel)\n",
    "        \n",
    "frames = []\n",
    "\n",
    "for start_frames in all_segments:\n",
    "    for frame in start_frames:\n",
    "        frames.append(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the dataframe storing data about RGB clips and EMG data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns=['uid', 'video_id', 'verb_class', 'label', 'start_frame', 'end_frame', 'emg_matrix'])\n",
    "\n",
    "count = 0\n",
    "starting_frames = []\n",
    "ending_frames = []\n",
    "uids = []\n",
    "video_ids_expanded = []\n",
    "\n",
    "for i, index in enumerate(frames):\n",
    "    for starting_frame in index:\n",
    "        ending_frame = starting_frame + np.int32(np.round(segment_duration_s * fps))\n",
    "        ending_frames.append(ending_frame)\n",
    "        starting_frames.append(starting_frame)\n",
    "        uids.append(count)\n",
    "        video_ids_expanded.append(video_ids[i])\n",
    "        count +=1\n",
    "\n",
    "dataframe[\"uid\"] = uids\n",
    "dataframe[\"video_id\"] = video_ids_expanded\n",
    "dataframe[\"verb_class\"] = example_label_indexes\n",
    "dataframe[\"label\"] = example_labels\n",
    "dataframe[\"start_frame\"] = starting_frames\n",
    "dataframe[\"stop_frame\"] = ending_frames\n",
    "dataframe[\"emg_matrix\"] = example_matrices\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframe in a Pickle file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_filepath, \"wb\") as f: pickle.dump(dataframe, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
